# Plan de Migration - Restructuration du Projet

## üéØ Objectif
R√©organiser le projet selon la structure optimis√©e d√©finie dans `ANALYSE_STRUCTURE_PROJET.md`.

---

## ‚ö†Ô∏è Pr√©requis

1. **Sauvegarder le projet actuel**
   ```bash
   # Cr√©er une branche de sauvegarde
   git checkout -b backup-before-restructure
   git add .
   git commit -m "Backup avant restructuration"
   ```

2. **V√©rifier les d√©pendances**
   - S'assurer que tous les scripts fonctionnent avant migration
   - Noter les chemins relatifs utilis√©s dans le code

---

## üìã Plan d'Ex√©cution par Phases

### Phase 1: Nettoyage Imm√©diat ‚ö°

#### 1.1 Supprimer les fichiers syst√®me macOS
```bash
# Supprimer les dossiers __MACOSX
Remove-Item -Recurse -Force "__MACOSX" -ErrorAction SilentlyContinue
Remove-Item -Recurse -Force "smarttech-streaming\__MACOSX" -ErrorAction SilentlyContinue
```

#### 1.2 Cr√©er la structure de base
```bash
# Cr√©er les nouveaux dossiers
New-Item -ItemType Directory -Force -Path "docs\terraform"
New-Item -ItemType Directory -Force -Path "docs\streaming"
New-Item -ItemType Directory -Force -Path "infrastructure\terraform"
New-Item -ItemType Directory -Force -Path "scripts"
```

#### 1.3 D√©placer la documentation Terraform
```bash
# D√©placer les PDFs
Move-Item "IaC-Provisionning(Terraform)\*.pdf" "docs\terraform\" -Force
# Supprimer l'ancien dossier
Remove-Item -Recurse -Force "IaC-Provisionning(Terraform)"
```

**‚úÖ Checklist Phase 1:**
- [ ] Dossiers __MACOSX supprim√©s
- [ ] Structure de base cr√©√©e
- [ ] Documentation Terraform d√©plac√©e
- [ ] .gitignore cr√©√© et fonctionnel

---

### Phase 2: R√©organisation du Projet Streaming üöÄ

#### 2.1 Aplatir la structure imbriqu√©e
```bash
# Se placer dans le dossier smarttech-streaming
cd smarttech-streaming

# D√©placer le contenu du sous-dossier vers le parent
Move-Item "smarttech-streaming\*" "." -Force
# Supprimer le dossier vide
Remove-Item -Recurse -Force "smarttech-streaming"
```

#### 2.2 Cr√©er la nouvelle structure Python
```bash
# Cr√©er les dossiers de la structure optimis√©e
New-Item -ItemType Directory -Force -Path "src\producers"
New-Item -ItemType Directory -Force -Path "src\streams"
New-Item -ItemType Directory -Force -Path "src\utils"
New-Item -ItemType Directory -Force -Path "notebooks"
New-Item -ItemType Directory -Force -Path "data\raw\sensor_data"
New-Item -ItemType Directory -Force -Path "data\raw\activity-data"
New-Item -ItemType Directory -Force -Path "data\raw\stream_events"
New-Item -ItemType Directory -Force -Path "data\input"
New-Item -ItemType Directory -Force -Path "data\delta"
New-Item -ItemType Directory -Force -Path "data\checkpoints"
New-Item -ItemType Directory -Force -Path "tests"
New-Item -ItemType Directory -Force -Path "config"
```

#### 2.3 D√©placer et r√©organiser les fichiers

**Scripts:**
```bash
# D√©placer et renommer les scripts
Move-Item "scripts\producer_kafka.py" "src\producers\kafka_producer.py"
Move-Item "scripts\stream_files_to_delta_bronze.py" "src\streams\bronze_stream.py"
Move-Item "scripts\stream_kafka_to_delta_silver.py" "src\streams\silver_stream.py"
```

**Notebooks:**
```bash
# D√©placer les notebooks
Move-Item "veille\*.ipynb" "notebooks\" -Force
```

**Donn√©es:**
```bash
# D√©placer les donn√©es de capteurs depuis la racine
Move-Item "..\sensor_data\*.json" "data\raw\sensor_data\" -Force

# D√©placer les donn√©es depuis veille
Move-Item "veille\activity-data\*.json" "data\raw\activity-data\" -Force
Move-Item "veille\stream_events\*.json" "data\raw\stream_events\" -Force

# D√©placer le dossier input s'il existe
if (Test-Path "input") {
    Move-Item "input\*" "data\input\" -Force
}
```

**Checkpoints et Delta:**
```bash
# D√©placer les checkpoints et delta existants
if (Test-Path "checkpoints") {
    Move-Item "checkpoints\*" "data\checkpoints\" -Force
}
if (Test-Path "delta") {
    Move-Item "delta\*" "data\delta\" -Force
}
```

#### 2.4 Cr√©er les fichiers __init__.py
```bash
# Cr√©er les fichiers __init__.py pour les modules Python
New-Item -ItemType File -Force -Path "src\__init__.py"
New-Item -ItemType File -Force -Path "src\producers\__init__.py"
New-Item -ItemType File -Force -Path "src\streams\__init__.py"
New-Item -ItemType File -Force -Path "src\utils\__init__.py"
New-Item -ItemType File -Force -Path "tests\__init__.py"
```

#### 2.5 Nettoyer les dossiers vides
```bash
# Supprimer les dossiers vides
Remove-Item -Recurse -Force "veille" -ErrorAction SilentlyContinue
Remove-Item -Recurse -Force "scripts" -ErrorAction SilentlyContinue
Remove-Item -Recurse -Force "input" -ErrorAction SilentlyContinue
```

**‚úÖ Checklist Phase 2:**
- [ ] Structure imbriqu√©e aplatie
- [ ] Nouvelle structure Python cr√©√©e
- [ ] Scripts d√©plac√©s et renomm√©s
- [ ] Notebooks d√©plac√©s
- [ ] Donn√©es organis√©es
- [ ] Fichiers __init__.py cr√©√©s
- [ ] Dossiers vides supprim√©s

---

### Phase 3: Refactorisation du Code üîß

#### 3.1 Cr√©er le module de configuration Spark

**Cr√©er `src/utils/spark_config.py`:**
```python
"""
Configuration Spark r√©utilisable pour les streams.
"""
import os
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip


def create_spark_session(app_name: str, extra_packages: list = None):
    """
    Cr√©e une session Spark configur√©e pour Delta Lake.
    
    Args:
        app_name: Nom de l'application Spark
        extra_packages: Liste de packages suppl√©mentaires (ex: Kafka)
    
    Returns:
        SparkSession configur√©e
    """
    # Nettoyage des variables d'env qui peuvent casser le bind du driver
    for var in ["SPARK_LOCAL_IP", "SPARK_DRIVER_BIND_ADDRESS", "SPARK_DRIVER_HOST"]:
        os.environ.pop(var, None)
    
    builder = (
        SparkSession.builder
        .appName(app_name)
        .master("local[*]")
        .config("spark.driver.bindAddress", "127.0.0.1")
        .config("spark.driver.host", "localhost")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    )
    
    if extra_packages:
        spark = configure_spark_with_delta_pip(builder, extra_packages=extra_packages).getOrCreate()
    else:
        spark = configure_spark_with_delta_pip(builder).getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark
```

#### 3.2 Cr√©er le module de sch√©mas

**Cr√©er `src/utils/schemas.py`:**
```python
"""
Sch√©mas de donn√©es pour les streams.
"""
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType


def get_sensor_schema():
    """
    Retourne le sch√©ma pour les donn√©es de capteurs.
    
    Returns:
        StructType: Sch√©ma Spark pour les √©v√©nements de capteurs
    """
    return StructType([
        StructField("timestamp", StringType(), True),
        StructField("device_id", StringType(), True),
        StructField("building", StringType(), True),
        StructField("floor", IntegerType(), True),
        StructField("type", StringType(), True),
        StructField("value", DoubleType(), True),
        StructField("unit", StringType(), True),
    ])


def get_sensor_schema_string():
    """
    Retourne le sch√©ma sous forme de string (pour JSON).
    
    Returns:
        str: Sch√©ma sous forme de string
    """
    return """
        timestamp string,
        device_id string,
        building string,
        floor int,
        type string,
        value double,
        unit string
    """
```

#### 3.3 Mettre √† jour les scripts de streaming

**Mettre √† jour `src/streams/bronze_stream.py`:**
- Importer depuis `src.utils.spark_config` et `src.utils.schemas`
- Utiliser les chemins relatifs mis √† jour (`data/input`, `data/delta`, etc.)

**Mettre √† jour `src/streams/silver_stream.py`:**
- Importer depuis `src.utils.spark_config` et `src.utils.schemas`
- Utiliser les chemins relatifs mis √† jour

**Mettre √† jour `src/producers/kafka_producer.py`:**
- V√©rifier les imports et chemins

#### 3.4 Mettre √† jour main.py
```python
"""
Point d'entr√©e principal de l'application SmartTech Streaming.
"""
from src.producers.kafka_producer import generate_event, KafkaProducer
# ... autres imports selon besoins

def main():
    print("SmartTech Streaming Application")
    # Logique principale

if __name__ == "__main__":
    main()
```

**‚úÖ Checklist Phase 3:**
- [ ] Module spark_config.py cr√©√©
- [ ] Module schemas.py cr√©√©
- [ ] Scripts de streaming mis √† jour
- [ ] Scripts de production mis √† jour
- [ ] main.py mis √† jour
- [ ] Tous les imports fonctionnent

---

### Phase 4: Mise √† Jour des Chemins et Configuration üìù

#### 4.1 Mettre √† jour les chemins dans les scripts

**Dans `src/streams/bronze_stream.py`:**
- `"input"` ‚Üí `"data/input"`
- `"checkpoints/bronze_sensors"` ‚Üí `"data/checkpoints/bronze_sensors"`
- `"delta/bronze_sensors"` ‚Üí `"data/delta/bronze_sensors"`

**Dans `src/streams/silver_stream.py`:**
- `"checkpoints/silver_sensors"` ‚Üí `"data/checkpoints/silver_sensors"`
- `"delta/silver_sensors"` ‚Üí `"data/delta/silver_sensors"`

#### 4.2 Cr√©er un .gitignore sp√©cifique au projet streaming

**Cr√©er `smarttech-streaming/.gitignore`:**
```gitignore
# Spark checkpoints et donn√©es Delta
data/checkpoints/
data/delta/
data/input/*.json

# Donn√©es brutes volumineuses (optionnel)
data/raw/stream_events/
data/raw/activity-data/

# Python
__pycache__/
*.pyc
.venv/
```

#### 4.3 Mettre √† jour docker-compose.yml
V√©rifier que les chemins et configurations sont toujours valides.

**‚úÖ Checklist Phase 4:**
- [ ] Tous les chemins mis √† jour dans les scripts
- [ ] .gitignore cr√©√© pour smarttech-streaming
- [ ] docker-compose.yml v√©rifi√©
- [ ] Configuration test√©e

---

### Phase 5: Tests et Validation ‚úÖ

#### 5.1 Tests de base
```bash
# Tester les imports Python
python -c "from src.utils import spark_config, schemas; print('OK')"

# V√©rifier la structure
tree /F smarttech-streaming
```

#### 5.2 Tests fonctionnels
1. Tester le producteur Kafka
2. Tester le stream Bronze
3. Tester le stream Silver
4. V√©rifier que les donn√©es sont bien √©crites dans Delta

#### 5.3 Documentation
- Mettre √† jour le README.md du projet streaming
- Documenter les nouveaux chemins
- Cr√©er un guide de d√©marrage rapide

**‚úÖ Checklist Phase 5:**
- [ ] Imports Python fonctionnent
- [ ] Structure valid√©e
- [ ] Tests fonctionnels pass√©s
- [ ] Documentation mise √† jour

---

## üö® Points d'Attention

1. **Chemins relatifs**: Tous les scripts doivent √™tre ex√©cut√©s depuis `smarttech-streaming/`
2. **Donn√©es existantes**: Les checkpoints et donn√©es Delta existants doivent √™tre migr√©s
3. **D√©pendances**: V√©rifier que `pyproject.toml` est √† jour
4. **Environnement**: S'assurer que l'environnement Python est correctement configur√©

---

## üìä Script de Migration Automatis√©

Un script PowerShell complet est disponible dans `scripts/migrate_structure.ps1` pour automatiser cette migration.

---

## ‚úÖ Validation Finale

Apr√®s migration, v√©rifier:

- [ ] Structure de dossiers conforme au plan
- [ ] Tous les fichiers √† leur nouvelle place
- [ ] Aucun fichier orphelin
- [ ] .gitignore fonctionne correctement
- [ ] Tests passent
- [ ] Documentation √† jour

---

*Plan cr√©√© le: $(Get-Date)*
*Derni√®re mise √† jour: Version initiale*

