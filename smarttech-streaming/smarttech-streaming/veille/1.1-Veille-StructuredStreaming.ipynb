{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b552473f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "from pyspark.sql.functions import col, from_json, to_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Netoyage des variables d'env qui peuvent casser le bind du driver\n",
        "for var in [\"SPARK_LOCAL_IP\", \"SPARK_DRIVER_BIND_ADDRESS\", \"SPARK_DRIVER_HOST\"]:\n",
        "    os.environ.pop(var, None)\n",
        "\n",
        "# Packages Kafka pour Spark (adapter la version si ton pyspark n'est pas 3.5.*)\n",
        "EXTRA_PACKAGES = [\n",
        "    \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.2\",\n",
        "]\n",
        "\n",
        "builder = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"SmartTech-Streaming-Silver\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
        "    .config(\"spark.driver.host\", \"localhost\")\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        ")\n",
        "\n",
        "# on passe Kafka via extra_packages\n",
        "spark = configure_spark_with_delta_pip(builder, extra_packages=EXTRA_PACKAGES).getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\") # permet de limiter les logs Spark à l’essentiel, en masquant les messages INFO et DEBUG\n",
        "# spark.sparkContext.setLogLevel(\"ALL\")\n",
        "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
        "# spark.sparkContext.setLogLevel(\"INFO\")\n",
        "# spark.sparkContext.setLogLevel(\"WARN\")   # le plus courant\n",
        "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "# spark.sparkContext.setLogLevel(\"FATAL\")\n",
        "# spark.sparkContext.setLogLevel(\"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ffa757",
      "metadata": {},
      "source": [
        "\n",
        "  <h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> 1 - Lecture d'un flux</h2>\n",
        " \n",
        "  La méthode `SparkSession.readStream` renvoie un `DataStreamReader` utilisé pour configurer le flux.\n",
        " \n",
        "  Il y a un certain nombre de points clés pour la configuration d'un `DataStreamReader` :\n",
        "  * Le schéma\n",
        "  * Le type de flux : Fichiers, Kafka, TCP/IP, etc.\n",
        "  * Configuration spécifique au type de flux\n",
        "    * Pour les fichiers, le type de fichier, le chemin vers les fichiers, le nombre maximum de fichiers, etc.\n",
        "    * Pour TCP/IP, l'adresse du serveur, le numéro de port, etc.\n",
        "    * Pour Kafka, l'adresse du serveur, le port, les topics, les partitions, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5d07a866",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataSchema = \"Arrival_Time long, Creation_Time long, Device string, Index long, Model string, User string, gt string, x double, y double, z double\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1895a17",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataPath = \"...\"\n",
        "  .readStream                       # Renvoie DataStreamReader\n",
        "  .option(\"maxFilesPerTrigger\", 1)  # Forcer le traitement d'un seul fichier par déclencheur\n",
        "  .schema(dataSchema)               # Requis pour tous les DataFrames en streaming\n",
        "  .json(dataPath)                   # Le répertoire source du flux et le type de fichier\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb30f92",
      "metadata": {},
      "source": [
        "Et avec le `DataFrame` initial, nous pouvons appliquer quelques transformations :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "18b28799",
      "metadata": {},
      "outputs": [],
      "source": [
        "streamingDF = (initialDF\n",
        "  .withColumnRenamed(\"Index\", \"User_ID\")  # Renommer une colonne\n",
        "  .drop(\"_corrupt_record\")                # Supprimer une colonne\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c45484b",
      "metadata": {},
      "source": [
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Écriture d'un flux</h2>\n",
        "\n",
        "`DataFrame.writeStream` renvoie un `DataStreamWriter` utilisé pour configurer la sortie du flux.\n",
        "\n",
        "Quelques paramètres pour la configuration de `DataStreamWriter` :\n",
        "* Nom de la requête (facultatif) - nom unique parmi toutes les requêtes actives dans le SQLContext associé.\n",
        "* Déclencheur (facultatif) - La valeur par défaut est `ProcessingTime(0`) et elle exécutera la requête aussi rapidement que possible.\n",
        "* Répertoire de point de contrôle (facultatif pour les éviers pub/sub)\n",
        "* Mode de sortie\n",
        "* Évier de sortie (sink) : File Sink , Kafka Sink, Memory Sink, Foreach Sink, ...\n",
        "\n",
        "Une fois la configuration terminée, nous pouvons déclencher le job avec un appel à `.start()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03afdbe4",
      "metadata": {},
      "source": [
        "\n",
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Fenêtrage</h2>\n",
        "\n",
        "Si nous utilisions un DataFrame statique pour produire un compte agrégé, nous pourrions utiliser `groupBy()` et `count()`.\n",
        "\n",
        "Dans un streaming DataFrame, nous accumulons les comptes dans une fenêtre glissante, répondant à des questions comme \"Combien d'enregistrements recevons-nous chaque seconde ?\"\n",
        "\n",
        "**Fenêtres glissantes** : Les fenêtres se chevauchent et un seul événement peut être agrégé dans plusieurs fenêtres.\n",
        "\n",
        "**Fenêtres fixes** : Les fenêtres ne se chevauchent pas et un seul événement sera agrégé dans une seule fenêtre.\n",
        "\n",
        "Illustration fenêtres glissante et fixe <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Guide de programmation de Structured Streaming</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3edf5bf7",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Déclencheurs\n",
        "\n",
        "Le déclencheur spécifie quand le système doit traiter le prochain ensemble de données.\n",
        "\n",
        "| Type de déclencheur                    | Exemple | Remarques |\n",
        "|----------------------------------------|-----------|-------------|\n",
        "| Non spécifié                           |  | _DEFAULT_ - La requête sera exécutée dès que le système aura terminé de traiter la requête précédente |\n",
        "| Micro-lots à intervalle fixe           | `.trigger(Trigger.ProcessingTime(\"6 hours\"))` | La requête sera exécutée en micro-lots et lancée aux intervalles spécifiés par l'utilisateur |\n",
        "| Micro-lot unique                       | `.trigger(Trigger.Once())` | La requête exécutera _un seul_ micro-lot pour traiter toutes les données disponibles, puis s'arrêtera d'elle-même |\n",
        "| Continu avec intervalle de point de contrôle fixe | `.trigger(Trigger.Continuous(\"1 second\"))` | La requête sera exécutée en mode de traitement continu à faible latence, <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing\" target = \"_blank\">mode de traitement continu</a>. \n",
        "\n",
        "Dans l'exemple ci-dessous, vous utiliserez un intervalle fixe de 3 secondes :\n",
        "\n",
        "`.trigger(Trigger.ProcessingTime(\"3 seconds\"))`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5df8e1",
      "metadata": {},
      "source": [
        "### Point de contrôle\n",
        "\n",
        "Un <b>point de contrôle</b> stocke l'état actuel de votre job de streaming dans un système de stockage fiable tel que Azure Blob Storage ou HDFS. Il ne stocke pas l'état de votre job de streaming dans le système de fichiers local d'un nœud de votre cluster.\n",
        "\n",
        "Avec les journaux d'écriture anticipée, un flux terminé peut être redémarré et il continuera là où il s'était arrêté.\n",
        "\n",
        "Pour activer cette fonctionnalité, vous devez simplement spécifier l'emplacement d'un répertoire de point de contrôle :\n",
        "\n",
        "`.option(\"checkpointLocation\", checkpointPath)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac3b24b",
      "metadata": {},
      "source": [
        "\n",
        "Points à considérer :\n",
        "* Si vous n'avez pas de répertoire de point de contrôle, lorsque le job de streaming s'arrête, vous perdez tout l'état de votre job de streaming et lors du redémarrage, vous recommencez à zéro.\n",
        "* Pour certains sinks, vous obtiendrez une erreur si vous ne spécifiez pas un répertoire de point de contrôle :<br/>\n",
        "`analysisException: 'checkpointLocation must be specified either through option(\"checkpointLocation\", ...)..`\n",
        "* Notez également que chaque job de streaming doit avoir son propre répertoire de point de contrôle : pas de partage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25600cb",
      "metadata": {},
      "source": [
        "\n",
        "### Modes de Sortie\n",
        "\n",
        "| Mode   | Exemple | Remarques |\n",
        "\n",
        "\n",
        "| **Complet** | `.outputMode(\"complete\")` | La table de résultats mise à jour entière est écrite dans le récepteur. L'implémentation individuelle du récepteur décide comment gérer l'écriture de la table entière. |\n",
        "\n",
        "\n",
        "| **Ajout** | `.outputMode(\"append\")`     | Seules les nouvelles lignes ajoutées à la table de résultats depuis le dernier déclenchement sont écrites dans le récepteur. |\n",
        "\n",
        "\n",
        "| **Mise à jour** | `.outputMode(\"update\")`     | Seules les lignes de la table de résultats qui ont été mises à jour depuis le dernier déclenchement seront sorties dans le récepteur. Depuis Spark 2.1.1 |\n",
        "\n",
        "Dans l'exemple ci-dessous, nous écrivons dans un répertoire Parquet qui ne supporte que le mode `append` :\n",
        "\n",
        "`dsw.outputMode(\"append\")`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9207bc48",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Éviers (sink) de Sortie\n",
        "\n",
        "`DataStreamWriter.format` accepte les valeurs suivantes, entre autres :\n",
        "\n",
        "| Évier de Sortie | Exemple                                          | Remarques |\n",
        "| --------------- | ------------------------------------------------ | --------- |\n",
        "| **Fichier**     | `dsw.format(\"parquet\")`, `dsw.format(\"csv\")`...  | Déverse la Table de Résultats dans un fichier. Supporte Parquet, json, csv, etc. |\n",
        "| **Kafka**       | `dsw.format(\"kafka\")`      | Écrit la sortie dans un ou plusieurs sujets dans Kafka |\n",
        "| **Console**     | `dsw.format(\"console\")`    | Imprime les données dans la console (utile pour le débogage) |\n",
        "| **Mémoire**     | `dsw.format(\"memory\")`     | Met à jour une table en mémoire, qui peut être interrogée via Spark SQL ou l'API DataFrame |\n",
        "| **foreach**     | `dsw.foreach(writer: ForeachWriter)` | C'est votre \"échappatoire\", vous permettant d'écrire votre propre type d'évier. |\n",
        "| **Delta**       | `dsw.format(\"delta\")`     | Un sink propriétaire |\n",
        "\n",
        "Dans l'exemple ci-dessous, nous allons ajouter des fichiers à un répertoire Parquet et ensuite Delta et spécifier son emplacement avec cet appel :\n",
        "\n",
        "`.format(\"parquet\").start(outputPathDir)`\n",
        "\n",
        "`.format(\"delta\").start(\"db.table\")`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62284159",
      "metadata": {},
      "source": [
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exemple Streaming</h2>\n",
        "\n",
        "Dans la cellule ci-dessous, nous écrivons des données d'une requête de streaming vers `outputPathDir`.\n",
        "\n",
        "Note :\n",
        "\n",
        "1. Nous donnons un nom à la requête via l'appel à `.queryName`\n",
        "\n",
        "2. Spark commence à exécuter des jobs une fois que nous appelons `.start`\n",
        "\n",
        "3. L'appel à `.start` renvoie un objet `StreamingQuery`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5e364f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/15 13:20:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "outputPathDir = \"/.../veille/output.parquet\" # Sous-répertoire résultats du streaming\n",
        "checkpointPath = \"/.../output.parquet.checkpoint\"    \n",
        "# Définition de la requête de streaming\n",
        "streamingQuery = (streamingDF                     \n",
        "  .writeStream                                    \n",
        "  .queryName(\"stream_1p\")                         \n",
        "  .trigger(processingTime=\"3 seconds\")            # déclencheur pour exécuter un micro-lot toutes les 3 secondes\n",
        "  .format(\"parquet\")                              # destination au format Parquet\n",
        "  .option(\"checkpointLocation\", checkpointPath)   # fichiers de point de contrôle\n",
        "  .outputMode(\"append\")                           # seules les nouvelles données seront écrites dans le fichier\n",
        "  .start(outputPathDir)                           # Démarre la requête et envoie les résultats dans le répertoire spécifié\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e2e0953",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "spark.sql(\"create database if not exists outputDelta;\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd15d25",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/15 13:25:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/12/15 13:25:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/12/15 13:25:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000} milliseconds, but spent 3117 milliseconds\n"
          ]
        }
      ],
      "source": [
        "#outputPathDir = \"/mnt/tmp/output.parquet\" \n",
        "#checkpointPath = \"/mnt/tmp/output.delta.checkpoint\"    \n",
        "\n",
        "outputPathDir = \"/..../veille/output.parquet\" # Sous-répertoire résultats du streaming\n",
        "checkpointPath = \"/...../output.parquet.checkpoint\"\n",
        "\n",
        "\n",
        "# Définition de la requête de streaming\n",
        "streamingQuery = (streamingDF                     \n",
        "  .writeStream                                    \n",
        "  .queryName(\"stream_2p\")                         \n",
        "  .trigger(processingTime=\"3 seconds\")            \n",
        "  .format(\"delta\")                             \n",
        "  .option(\"checkpointLocation\", checkpointPath)  \n",
        "  .outputMode(\"append\")                           \n",
        "  .toTable(\"outputDelta.table\")                           \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20ddc06",
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM outputDelta.table\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b735093",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Script complet</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5690bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Définir le shéma de données\n",
        "dataSchema = \"Arrival_Time long, Creation_Time long, Device string, Index long, Model string, User string, gt string, x double, y double, z double\"\n",
        "\n",
        "# Lecture source de données\n",
        "dataPath = \"/databricks-datasets/definitive-guide/data/activity-data\"\n",
        "initialDF = (spark\n",
        "  .readStream                       # Renvoie DataStreamReader\n",
        "  .option(\"maxFilesPerTrigger\", 1)  # Forcer le traitement d'un seul fichier par déclencheur\n",
        "  .schema(dataSchema)               # Requis pour tous les DataFrames en streaming\n",
        "  .json(dataPath)                   # Le répertoire source du flux et le type de fichier\n",
        ")\n",
        "\n",
        "# Petite transformation\n",
        "streamingDF = (initialDF\n",
        "  .withColumnRenamed(\"Index\", \"User_ID\")  # Choisir un nom de colonne \"meilleur\"\n",
        "  .drop(\"_corrupt_record\")                # Supprimer une colonne inutile\n",
        ")\n",
        "\n",
        "\n",
        "# Ecriture du streaming dans un fichier au format Parquet\n",
        "outputPathDir = \"/mnt/tmp/output.parquet\" \n",
        "checkpointPath = \"/mnt/tmp/output.parquet.checkpoint\"  \n",
        "# Définition de la requête de streaming\n",
        "streamingQuery = (streamingDF                     \n",
        "  .writeStream                                    \n",
        "  .queryName(\"stream_1p\")                       \n",
        "  .trigger(processingTime=\"3 seconds\")            \n",
        "  .format(\"parquet\")                            \n",
        "  .option(\"checkpointLocation\", checkpointPath)   \n",
        "  .outputMode(\"append\")                           \n",
        "  .start(outputPathDir)                           \n",
        ")\n",
        "\n",
        "\n",
        "# Ecriture du streaming dans une table Delta\n",
        "outputPathDir = \"/mnt/tmp/output.parquet\" # Sous-répertoire où les résultats du streaming seront enregistrés au format Parquet\n",
        "checkpointPath = \"/mnt/tmp/output.delta.checkpoint\"    # Sous-répertoire pour les fichiers de point de contrôle\n",
        "# Définition de la requête de streaming\n",
        "streamingQuery = (streamingDF                     \n",
        "  .writeStream                                    \n",
        "  .queryName(\"stream_2p\")                         \n",
        "  .trigger(processingTime=\"3 seconds\")            \n",
        "  .format(\"delta\")                             \n",
        "  .option(\"checkpointLocation\", checkpointPath)  \n",
        "  .outputMode(\"append\")                           \n",
        "  .toTable(\"outputDelta.table\")                           \n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d05571",
      "metadata": {},
      "source": [
        "%md\n",
        "Liste flux actifs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ad446101",
      "metadata": {},
      "outputs": [],
      "source": [
        "for stream in spark.streams.active:      # Loop over all active streams\n",
        "    print(\" {} ({})\".format(stream.name, stream.id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469b9e6c",
      "metadata": {},
      "source": [
        "%md\n",
        "Arretter les flux actifs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2a2fce68",
      "metadata": {},
      "outputs": [],
      "source": [
        "for stream in spark.streams.active:\n",
        "  stream.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf8ad5f",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Résumé</h2>\n",
        "\n",
        "Nous utilisons `readStream` pour lire les entrées en streaming à partir de diverses sources et créer un DataFrame.\n",
        "\n",
        "Rien ne se passe jusqu'à ce que nous invoquions `writeStream` ou `display`.\n",
        "\n",
        "En utilisant `writeStream`, nous pouvons écrire vers une variété de récepteurs de sortie. En utilisant `display`, nous dessinons des graphiques en barres, des graphiques et d'autres types de tracés en direct dans le notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea1b14f3",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Questions de révision</h2>\n",
        "\n",
        "**Q:** Que font `readStream` et `writeStream` ?<br>\n",
        "\n",
        "\n",
        "**Q:** Que produit `display` s'il est appliqué à un DataFrame créé via `readStream` ?<br>\n",
        "\n",
        "**Q:** Lorsque vous exécutez une commande de flux d'écriture, que fait cette option `outputMode(\"append\")` ?<br>\n",
        "\n",
        "**Q:** Que se passe-t-il si vous ne spécifiez pas `option(\"checkpointLocation\", pointer-to-checkpoint directory)` ?<br>\n",
        "\n",
        "\n",
        "**Q:** Comment visualiser la liste des flux actifs ?<br>\n",
        "\n",
        "\n",
        "**Q:** Comment vérifier si `streamingQuery` est en cours d'exécution (sortie booléenne) ?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c707cf0d",
      "metadata": {},
      "source": [
        "%md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602b6afa",
      "metadata": {},
      "source": [
        "%md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1eaebc",
      "metadata": {},
      "source": [
        "%md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8c9054",
      "metadata": {},
      "source": [
        "%md"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b611a6",
      "metadata": {},
      "source": [
        "%md\n",
        "\n",
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Questions de révision</h2>\n",
        "\n",
        "**Q:** Que font `readStream` et `writeStream` ?<br>\n",
        "**A:** `readStream` crée un DataFrame en streaming.<br>`writeStream` envoie les données en streaming vers un récepteur de sortie.\n",
        "\n",
        "**Q:** Que produit `display` s'il est appliqué à un DataFrame créé via `readStream` ?<br>\n",
        "**A:** `display` envoie les données en streaming vers un graphique EN DIRECT !\n",
        "\n",
        "**Q:** Lorsque vous exécutez une commande de flux d'écriture, que fait cette option `outputMode(\"append\")` ?<br>\n",
        "**A:** Cette option prend les valeurs suivantes et leurs significations respectives :\n",
        "* <b>append</b> : ajouter uniquement de nouveaux enregistrements au récepteur de sortie\n",
        "* <b>complete</b> : réécrire la sortie complète - applicable aux opérations d'agrégation\n",
        "* <b>update</b> : mettre à jour les enregistrements modifiés sur place\n",
        "\n",
        "**Q:** Que se passe-t-il si vous ne spécifiez pas `option(\"checkpointLocation\", pointer-to-checkpoint directory)` ?<br>\n",
        "**A:** Lorsque le travail de streaming s'arrête, vous perdez tout l'état de votre job de streaming et lors du redémarrage, vous recommencez à zéro.\n",
        "\n",
        "**Q:** Comment visualiser la liste des flux actifs ?<br>\n",
        "**A:** Invoquez `spark.streams.active`.\n",
        "\n",
        "**Q:** Comment vérifier si `streamingQuery` est en cours d'exécution (sortie booléenne) ?<br>\n",
        "**A:** Invoquez `spark.streams.get(streamingQuery.id).isActive`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "smarttech-streaming",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
